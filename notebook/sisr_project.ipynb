{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a3dcb4",
   "metadata": {},
   "source": [
    "# Single Image Super Resolution using CNN and GAN\n",
    "Minor Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5515c",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed1e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffbe1a",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation (LRâ€“HR Image Pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b878a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, image_paths, scale=4):\n",
    "        self.image_paths = image_paths\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read HR image\n",
    "        hr = cv2.imread(self.image_paths[idx])\n",
    "        hr = cv2.cvtColor(hr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # FORCE SAME SIZE (VERY IMPORTANT)\n",
    "        hr = cv2.resize(hr, (256, 256))\n",
    "\n",
    "        h, w, _ = hr.shape\n",
    "\n",
    "        # Create LR image\n",
    "        lr = cv2.resize(hr, (w // self.scale, h // self.scale))\n",
    "        lr = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Normalize\n",
    "        hr = hr / 255.0\n",
    "        lr = lr / 255.0\n",
    "\n",
    "        # To tensor\n",
    "        hr = torch.tensor(hr).permute(2, 0, 1).float()\n",
    "        lr = torch.tensor(lr).permute(2, 0, 1).float()\n",
    "\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89c682",
   "metadata": {},
   "source": [
    "## 3. SRCNN Model (Baseline CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a81143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "def psnr(pred, target):\n",
    "    mse = nn.functional.mse_loss(pred, target)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c660c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRCNN Epoch 1/5, Loss: 0.143988\n",
      "SRCNN Epoch 2/5, Loss: 0.117418\n",
      "SRCNN Epoch 3/5, Loss: 0.086303\n",
      "SRCNN Epoch 4/5, Loss: 0.055813\n",
      "SRCNN Epoch 5/5, Loss: 0.032425\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---- DEFINE IMAGE PATHS (THIS IS THE ANSWER TO YOUR CONFUSION) ----\n",
    "image_paths = [\n",
    "    r\"C:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\images\\hr1.jpg\",\n",
    "    r\"C:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\images\\hr2.jpg\",\n",
    "    r\"C:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\images\\hr3.jpg\",\n",
    "    r\"C:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\images\\hr4.jpg\",\n",
    "    r\"C:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\images\\hr5.jpg\"\n",
    "    \n",
    "]\n",
    "\n",
    "dataset = SRDataset(image_paths, scale=4)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# ---- MODEL ----\n",
    "srcnn = SRCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(srcnn.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for lr, hr in loader:\n",
    "        sr = srcnn(lr)\n",
    "        loss = criterion(sr, hr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"SRCNN Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(loader):.6f}\")\n",
    "# ---- END OF CODE ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9699591",
   "metadata": {},
   "source": [
    "## 4. SRGAN Model (Generative Super Resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997fdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.residuals = nn.Sequential(*[ResidualBlock(64) for _ in range(5)])\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 3, 9, padding=4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.residuals(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(vgg.features.children())[:36])\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a7cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\Single-Image-Super-Resolution\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KIIT0001\\Videos\\Minor Proj 6th sem\\Single-Image-Super-Resolution\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRGAN Epoch 1\n",
      "SRGAN Epoch 2\n",
      "SRGAN Epoch 3\n"
     ]
    }
   ],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "content_loss = nn.MSELoss()\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "vgg = vgg19(pretrained=True).features[:36].eval()\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "opt_G = optim.Adam(G.parameters(), lr=1e-4)\n",
    "opt_D = optim.Adam(D.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for lr, hr in loader:\n",
    "        fake_hr = G(lr)\n",
    "\n",
    "        real_pred = D(hr)\n",
    "        fake_pred = D(fake_hr.detach())\n",
    "\n",
    "        d_loss = adversarial_loss(real_pred, torch.ones_like(real_pred)) + \\\n",
    "                 adversarial_loss(fake_pred, torch.zeros_like(fake_pred))\n",
    "\n",
    "        opt_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        fake_pred = D(fake_hr)\n",
    "        hr_resized = torch.nn.functional.interpolate(\n",
    "        hr, size=fake_hr.shape[2:], mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        perceptual = content_loss(vgg(fake_hr), vgg(hr_resized))\n",
    "\n",
    "\n",
    "        g_loss = content_loss(fake_hr, hr_resized) + \\\n",
    "                 1e-3 * adversarial_loss(fake_pred, torch.ones_like(fake_pred)) + \\\n",
    "                 0.006 * perceptual\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "    print(f\"SRGAN Epoch {epoch+1}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8b316",
   "metadata": {},
   "source": [
    "## 5. Results and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d91b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.08338928..0.43917587].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.08302008..0.5069683].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.09568811..0.50984436].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.115445755..0.53162223].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.07253943..0.39773285].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "srcnn.eval()\n",
    "G.eval()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    lr, hr = dataset[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        srcnn_sr = srcnn(lr.unsqueeze(0)).squeeze(0)\n",
    "        srgan_sr = G(lr.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12,4))\n",
    "    titles = [\"Low-Res\", \"SRCNN\", \"SRGAN\", \"High-Res\"]\n",
    "    images = [\n",
    "        lr.permute(1,2,0),\n",
    "        srcnn_sr.permute(1,2,0),   \n",
    "        srgan_sr.permute(1,2,0),\n",
    "        hr.permute(1,2,0)\n",
    "    ]\n",
    "\n",
    "    for j in range(4):\n",
    "        axs[j].imshow(images[j])\n",
    "        axs[j].set_title(titles[j])\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    plt.savefig(f\"results/result_{i+1}.png\")\n",
    "    plt.close()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
